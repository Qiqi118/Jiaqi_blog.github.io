# FLASH: Towards a High-performance Hardware Acceleration Architecture for Cross-silo Federated Learning

# Abstract

        跨职能团队联邦学习（Cross-silo federated learning）采用各种加密操作来保护数据隐私，这会带来显著的性能开销。我们确定了九种广泛使用的密码操作，并设计了一种高效的硬件架构来加速它们。

        然而，直接将他们卸载到硬件上会导致：

        1） 由于分配给每个操作的资源有限，硬件加速不足；

        2）不同的操作在不同的时间使用不同的资源，因此导致资源利用率不足。

        为了应对这些挑战，我们提出了FLASH，这是一种用于跨职能团队联邦学习的高性能硬件加速架构。FLASH的核心是提取两个基本运算符——幂运算和乘法运算——并将其作为高性能引擎来实现，以实现足够的加速。

# 1 Introduction

        训练一个高质量的机器学习模型需要大量的数据，这些数据很可能分布在现实世界中的不同机构或公司之间。然而，对数据隐私的日益担忧以及新出现的法规和诉讼限制了将这些数据集中在一个地方进行集中培训。为了解决这个问题，已经提出了联邦学习（FL），通过在数据竖井内执行局部计算并安全地聚合中间结果（例如，梯度/参数）来生成全局模型，而不向外界透露任何原始数据，从而实现这些数据竖井之间的分布式学习。

        为了确保跨职能团队FL的安全性，已经使用了各种加密技术。例如，部分同态加密（partially homomorphic encryptions，PHE），例如Paillier，已被用于实现直接对密文的参数计算/聚合[73]。RSA用于构建基于盲签名的私有集交集（PSI），用于样本比对[45]。我们对现有的跨职能团队FL应用程序进行了全面分析，并确定了九种使用广泛的加密操作，如加密/解密、密文计算等（更多细节见§3.1）。在保护隐私的同时，这些加密操作会显著降低性能（§3.2）。

        加密操作降低性能的原因：

        1）很高的计算复杂度，例如Paillier加密的时间复杂度有$O(2^N)$；

        2）引入了大量的计算，HE和RSA加密生成2048位密文，这些密文需要分解为多个64位数字，并在当前CPU架构上以有限的并行度执行。

        在本文中，我们要问：*我们能否将这些加密操作卸载到专用硬件上，以加速跨职能团队FL？*
