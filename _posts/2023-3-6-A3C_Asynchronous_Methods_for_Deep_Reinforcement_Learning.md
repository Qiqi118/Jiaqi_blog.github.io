# Asynchronous Methods for Deep Reinforcement Learning

# Abstract

        提出了一种概念上简单且轻量级的深度强化学习框架，该框架使用异步梯度下降来优化深度神经网络控制器。我们提出了四种标准强化学习算法的异步变体，并表明并行参与者学习者对训练具有稳定效果，从而允许所有四种方法成功地训练神经网络控制器。最佳性能的方法是Actor-Critic的异步变体，它超越了Atari领域目前的最先进技术，同时在单个多核CPU而不是GPU上训练了一半的时间。此外，我们还展示了异步演员评论家在各种连续电机控制问题上以及在使用视觉输入导航随机3D迷宫的新任务上的成功。

# 1 Introduction

        我们在环境的多个实例上并行异步执行多个代理，而不是体验回放。这种并行性还将代理的数据去关联到一个更稳定的过程中，因为在任何给定的时间步，并行代理都将经历各种不同的状态。这一简单的想法使得能够使用深度神经网络稳健有效地应用更广泛的基本策略上RL算法，如Sarsa、n步方法和actocritic方法，以及非策略RL算法，例如Q-Learning。

        前面的DRL算法严重依赖GPU等专用硬件或是大规模分布式架构，而我们的实验在具有标准多核CPU的单机上运行。我们认为，A3C在2D和3D游戏、离散和连续动作空间上的成功，以及其训练前馈和递归代理的能力，使其成为迄今为止最成功的强化学习代理。

# 2 Related Work

        （Nair等人，2015）提出的通用强化学习架构（General Reinforcement Learning Architecture，Gorila）在分布式环境中执行强化学习代理的异步训练。在Gorila中，每个过程都包含一个参与者，该参与者在其自己的环境副本中发挥作用，一个单独的重放存储器，以及一个学习者，该学习者从重放存储器中采样数据并计算DQN损失相对于策略参数的梯度。梯度被异步发送到中心参数服务器，该服务器更新模型的中心副本。更新的策略参数以固定的间隔发送给参与者学习者。通过使用100个独立的演员学习器过程和30个参数服务器实例，总共130台机器，Gorila能够在49个Atari游戏中显著超过DQN。在许多比赛中，Gorila比DQN快20倍达到了DQN的得分。我们还注意到（Chavez等人，2015）提出了一种类似的DQN并行化方法。

        在早期的工作中，（Li&Schuurmans，2011）将Map Reduce框架应用于使用线性函数逼近并行化批量强化学习方法。并行性用于加速大型矩阵运算，但不用于并行收集经验或稳定学习。（Grounds&Kudenko，2008）提出了一种并行版本的Sarsa算法，该算法使用多个独立的演员学习者来加速训练。每个参与者学习者分别学习，并使用对等通信定期向其他学习者发送权重显著变化的更新。

# 3 RL Background

这里大概讲了RL的基本流程


