# QUASI-RECURRENT NEURAL NETWORKS

# Abstract

        递归神经网络是对序列数据建模的强大工具，但每个时间步的计算对前一个时间步的输出的依赖性限制了并行性，并使RNN在很长的序列中难以使用。我们引入了准递归神经网络（QRNNs），这是一种交替卷积层的神经序列建模方法，在时间步长上并行应用，以及在通道上并行应用的最小递归池函数。尽管缺乏可训练的递归层，但堆叠的QRNN比相同隐藏大小的堆叠LSTM具有更好的预测精度。由于它们的平行度增加，它们在训练和测试时的速度高达16倍。语言建模、情感分类和字符级神经机器翻译的实验证明了这些优势，并强调了QRNN作为各种序列任务的基本构建块的可行性。

# Introduction

递归神经网络（RNN），包括长短期记忆（LSTM）等变体已成为序列建模任务的深度学习方法的标准模型架构。RNN将具有可训练参数的函数重复应用于隐藏状态。递归层也可以堆叠，从而增加网络深度、代表性和准确性。RNN在自然语言领域的应用范围从句子分类（Wang et al.，2015）到单词和字符级别的语言建模（Zaremba et al.，2014）。RNN通常也是机器翻译等任务的更复杂模型的基本构建块（Bahdanau等人，2015；Luong等人，2015年；Bradbury&Socher，2016）或问答（Kumar等人，2016；Xiong等人，2016）。不幸的是，包括LSTM在内的标准RNN在处理涉及很长序列的任务（如文档分类或字符级机器翻译）的能力方面受到限制，因为无法并行计算文档不同部分的特征或状态。
